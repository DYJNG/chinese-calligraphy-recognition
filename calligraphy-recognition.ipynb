{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 书法识别竞赛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyjng/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet import init\n",
    "from mxnet.gluon.data import vision\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from mxnet.gluon import nn\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 120\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "#     transforms.CenterCrop(32),\n",
    "#     transforms.RandomFlipTopBottom(),\n",
    "#     transforms.RandomColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.0),\n",
    "#     transforms.RandomLighting(0.0),\n",
    "#     transforms.Cast('float32'),\n",
    "#     transforms.Resize(32),\n",
    "    transforms.Resize(224),\n",
    "#     transforms.Resize(227),\n",
    "    \n",
    "    # 随机按照 scale 和 ratio 裁剪， 并放缩为 227*227 的正方形\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0)),\n",
    "#     transforms.RandomResizedCrop(32, scale=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0)),\n",
    "#     transforms.RandomFlipLeftRight(),\n",
    "    # 将像素值缩小到 (0, 1) 内， 并将数据格式从 “ 高 × 宽 × 通道 ” 改为 “ 通道 × 高 × 宽”\n",
    "    transforms.ToTensor()\n",
    "    # 对图片的每个通道做标准化 --减去均值，除以方差\n",
    "#     transforms.Normalize(mean=0.4914, std=0.2023),\n",
    "    \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "#     transforms.Resize(227),\n",
    "#     transforms.Resize(32),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(mean=0.4914, std=0.2023)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vision.ImageFolderDataset??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/calligraphy/'\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 读取原始图像文件， flag = 0 表示图像有 1 个通道（灰度图）\n",
    "train_ds = vision.ImageFolderDataset(data_dir + 'train', flag=0)\n",
    "test_ds = vision.ImageFolderDataset(data_dir + 'test', flag=0)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds.transform_first(transform_train), \n",
    "                    batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds.transform_first(transform_test), \n",
    "                    batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "16343\n",
      "625\n",
      "(64, 1, 224, 224) (64,)\n",
      "\n",
      "[42 51 88 25 77  0 29 94 35 95 51 84 20 89 27 42 56 22 81 22 14  3 89 91 79\n",
      " 75 81 91 66 66 26 55 50 31 45 78 60 99 53 13 37 41 10 96 58 57 82 95 77 85\n",
      " 23 45 61 97 23 46 34 15 39 24 88 59 17 18]\n",
      "<NDArray 64 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(len(test_ds))\n",
    "\n",
    "print(len(train_data))\n",
    "for data, label in train_data:\n",
    "    print(data.shape, label.shape)\n",
    "    print(label.as_in_context(mx.gpu(0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设计模型 --AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(AlexNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # conv1\n",
    "            net.add(nn.Conv2D(channels=96, kernel_size=11, \n",
    "                              strides=4, padding=0))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            net.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "            # conv2\n",
    "            net.add(nn.Conv2D(channels=256, kernel_size=5, \n",
    "                              strides=1, padding=2))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            net.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "            # conv3\n",
    "            net.add(nn.Conv2D(channels=384, kernel_size=3, \n",
    "                              strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # conv4\n",
    "            net.add(nn.Conv2D(channels=384, kernel_size=3, \n",
    "                              strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # conv5\n",
    "            net.add(nn.Conv2D(channels=256, kernel_size=3, \n",
    "                              strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            net.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "            \n",
    "            # FC\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(4096))\n",
    "            net.add(nn.Dropout(0.5))\n",
    "            net.add(nn.Dense(4096))\n",
    "            net.add(nn.Dropout(0.5))\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):   # __init__() 里的 self\n",
    "        out = x\n",
    "        for i, f in enumerate(self.net):\n",
    "            out = f(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d Output: %s' % (i+1, out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape = True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size = 3, \n",
    "                                   padding = 1, strides = strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size = 3, \n",
    "                                   padding = 1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size = 1, \n",
    "                                       strides = strides)\n",
    "            \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose = False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # modual 1\n",
    "            net.add(nn.Conv2D(channels = 32, kernel_size = 3, \n",
    "                              strides = 1, padding = 1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation = 'relu'))\n",
    "            net.add(nn.Dropout(0.2))\n",
    "            # modual 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels = 32))\n",
    "                net.add(nn.Dropout(0.2))\n",
    "            # modual 3\n",
    "            net.add(Residual(channels = 64, same_shape = False))\n",
    "            net.add(nn.Dropout(0.2))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels = 64))\n",
    "                net.add(nn.Dropout(0.2))\n",
    "            # modual 4\n",
    "            net.add(Residual(channels = 128, same_shape = False))\n",
    "            net.add(nn.Dropout(0.2))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels = 128))\n",
    "                net.add(nn.Dropout(0.2))\n",
    "            # modual 5\n",
    "            net.add(nn.AvgPool2D(pool_size = 8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "        \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s' % (i+1, out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class residual_block(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, first_residual=False, **kwargs):\n",
    "        super(residual_block, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        self.first_residual = first_residual\n",
    "        strides = 1 if same_shape else 2\n",
    "        with self.name_scope():\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv1 = nn.Conv2D(channels=channels, kernel_size=1, \n",
    "                                   strides=strides, padding=0)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels=channels, kernel_size=3, \n",
    "                                   strides=1, padding=1)\n",
    "            self.bn3 = nn.BatchNorm()\n",
    "            self.conv3 = nn.Conv2D(channels=4*channels, kernel_size=1, \n",
    "                                   strides=1, padding=0)\n",
    "            if not same_shape:\n",
    "                self.conv4 = nn.Conv2D(channels=4*channels, kernel_size=1, \n",
    "                                       strides=strides, padding=0)\n",
    "            elif first_residual:\n",
    "                self.conv4 = nn.Conv2D(channels=4*channels, kernel_size=1, \n",
    "                                       strides=strides, padding=0)\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = F.relu(self.bn1(x))\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        if not self.same_shape or self.first_residual:\n",
    "            x = self.conv4(x)\n",
    "        return out + x\n",
    "    \n",
    "class ResNet50(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet50, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # stage 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=7, \n",
    "                              strides=2, padding=3))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            net.add(nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "            # stage 2\n",
    "            net.add(residual_block(64, first_residual=True))\n",
    "            for _ in range(2):\n",
    "                net.add(residual_block(64))\n",
    "            # stage 3\n",
    "            net.add(residual_block(128, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(residual_block(128))\n",
    "            # stage 4\n",
    "            net.add(residual_block(256, same_shape=False))\n",
    "            for _ in range(5):\n",
    "                net.add(residual_block(256))\n",
    "            # stage 5\n",
    "            net.add(residual_block(512, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(residual_block(512))\n",
    "            # stage 6\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            net.add(nn.AvgPool2D(pool_size=7))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "            \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, f in enumerate(self.net):\n",
    "            out = f(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d, Output: %s' % (i+1, out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net(ctx, num_classes=100):\n",
    "#     net = AlexNet(num_classes)\n",
    "#     net = ResNet(num_classes)\n",
    "    net = ResNet50(num_classes)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_data, valid_data, net, ctx, num_epoches, \n",
    "          learning_rate=0.01, lr_decay=0.1, lr_period=50, \n",
    "          momentum = 0.9, weight_decay=0, cost_period = 10, \n",
    "          print_cost=False):\n",
    "    costs = []\n",
    "#     trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "#                             {'learning_rate': learning_rate, \n",
    "#                              'momentum': momentum, \n",
    "#                              'wd': weight_decay})\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', \n",
    "                            {'learning_rate': learning_rate})\n",
    "    pre_time = datetime.datetime.now()\n",
    "#     moving_loss = 0\n",
    "#     niter = 0\n",
    "    for epoch in range(num_epoches):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        if (epoch+1) % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            data = data.as_in_context(ctx)\n",
    "#             label = label.as_in_context(ctx)\n",
    "            label = label.astype('float32').as_in_context(ctx)\n",
    "            with ag.record():\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "#             print(output.argmax(axis=1).astype(np.int64), label)\n",
    "            train_acc += nd.mean(output.argmax(axis=1) == label).asscalar()\n",
    "#             train_acc += nd.mean(output.argmax(axis=1).astype(np.int64) == label).asscalar()\n",
    "#             niter += 1\n",
    "#             cur_loss = nd.mean(loss).asscalar()\n",
    "#             moving_loss = 0.9 * moving_loss + 0.1 * cur_loss\n",
    "#             corr_loss = moving_loss / pow(0.9, niter)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - pre_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc = 0\n",
    "            for data, label in valid_data:\n",
    "                data = data.as_in_context(ctx)\n",
    "#                 label = label.as_in_context(ctx)\n",
    "                label = label.astype('float32').as_in_context(ctx)\n",
    "                output = net(data)\n",
    "#                 valid_acc += nd.mean(output.argmax(axis=1).astype(np.int64) == label).asscalar()\n",
    "                valid_acc += nd.mean(output.argmax(axis=1) == label).asscalar()\n",
    "            epoch_str = \"Epoch %d, train_loss: %f, train_acc: %f, valid_acc %f, \" % (epoch+1, \n",
    "                                                                                   train_loss/len(train_data), \n",
    "                                                                                   train_acc/len(train_data), \n",
    "                                                                                   valid_acc/len(valid_data))\n",
    "        else:\n",
    "            epoch_str = \"Epoch %d, train_loss: %f, train_acc: %f, \" % (epoch+1, \n",
    "                                                                     train_loss/len(train_data), \n",
    "                                                                     train_acc/len(train_data))\n",
    "        if print_cost and (epoch+1) % cost_period == 0:\n",
    "#             costs.append(corr_loss)\n",
    "            costs.append(train_loss/len(train_data))\n",
    "        print(epoch_str + time_str + ', lr: %f' % trainer.learning_rate)\n",
    "        pre_time = cur_time\n",
    "    if print_cost:\n",
    "        x_axis = np.linspace(0, num_epoches, len(costs), endpoint = True)\n",
    "        plt.semilogy(x_axis, costs)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_data = None\n",
    "ctx = mx.gpu(0)\n",
    "num_epoches = 200\n",
    "learning_rate = 0.01\n",
    "lr_decay = 0.1\n",
    "lr_period = 50\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "cost_period = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 4.666875, train_acc: 0.013600, Time 00:03:44, lr: 0.010000\n",
      "Epoch 2, train_loss: 4.209906, train_acc: 0.055575, Time 00:03:43, lr: 0.010000\n",
      "Epoch 3, train_loss: 2.967842, train_acc: 0.271750, Time 00:03:41, lr: 0.010000\n",
      "Epoch 4, train_loss: 1.842795, train_acc: 0.534200, Time 00:03:41, lr: 0.010000\n",
      "Epoch 5, train_loss: 1.267472, train_acc: 0.678775, Time 00:03:41, lr: 0.010000\n",
      "Epoch 6, train_loss: 0.982198, train_acc: 0.748200, Time 00:03:41, lr: 0.010000\n",
      "Epoch 7, train_loss: 0.787672, train_acc: 0.799150, Time 00:03:44, lr: 0.010000\n",
      "Epoch 8, train_loss: 0.646470, train_acc: 0.831275, Time 00:03:42, lr: 0.010000\n",
      "Epoch 9, train_loss: 0.540609, train_acc: 0.858325, Time 00:03:42, lr: 0.010000\n",
      "Epoch 10, train_loss: 0.469920, train_acc: 0.876700, Time 00:03:41, lr: 0.010000\n",
      "Epoch 11, train_loss: 0.399983, train_acc: 0.890975, Time 00:03:41, lr: 0.010000\n",
      "Epoch 12, train_loss: 0.333821, train_acc: 0.906775, Time 00:03:41, lr: 0.010000\n",
      "Epoch 13, train_loss: 0.281528, train_acc: 0.919125, Time 00:03:42, lr: 0.010000\n",
      "Epoch 14, train_loss: 0.241589, train_acc: 0.929400, Time 00:03:41, lr: 0.010000\n",
      "Epoch 15, train_loss: 0.199406, train_acc: 0.942025, Time 00:03:42, lr: 0.010000\n",
      "Epoch 16, train_loss: 0.178450, train_acc: 0.946975, Time 00:03:42, lr: 0.010000\n",
      "Epoch 17, train_loss: 0.150981, train_acc: 0.953575, Time 00:03:42, lr: 0.010000\n",
      "Epoch 18, train_loss: 0.136093, train_acc: 0.959225, Time 00:03:42, lr: 0.010000\n",
      "Epoch 19, train_loss: 0.116490, train_acc: 0.964450, Time 00:03:41, lr: 0.010000\n",
      "Epoch 20, train_loss: 0.098449, train_acc: 0.969300, Time 00:03:41, lr: 0.010000\n",
      "Epoch 21, train_loss: 0.100173, train_acc: 0.968125, Time 00:03:42, lr: 0.010000\n",
      "Epoch 22, train_loss: 0.085504, train_acc: 0.972650, Time 00:03:42, lr: 0.010000\n",
      "Epoch 23, train_loss: 0.085329, train_acc: 0.972550, Time 00:03:42, lr: 0.010000\n",
      "Epoch 24, train_loss: 0.084979, train_acc: 0.972225, Time 00:03:42, lr: 0.010000\n",
      "Epoch 25, train_loss: 0.058676, train_acc: 0.982600, Time 00:03:41, lr: 0.010000\n",
      "Epoch 26, train_loss: 0.072099, train_acc: 0.977025, Time 00:03:41, lr: 0.010000\n",
      "Epoch 27, train_loss: 0.062826, train_acc: 0.980475, Time 00:03:41, lr: 0.010000\n",
      "Epoch 28, train_loss: 0.062347, train_acc: 0.979675, Time 00:03:41, lr: 0.010000\n",
      "Epoch 29, train_loss: 0.059204, train_acc: 0.980950, Time 00:03:41, lr: 0.010000\n",
      "Epoch 30, train_loss: 0.057328, train_acc: 0.981800, Time 00:03:41, lr: 0.010000\n",
      "Epoch 31, train_loss: 0.058235, train_acc: 0.980875, Time 00:03:41, lr: 0.010000\n",
      "Epoch 32, train_loss: 0.043492, train_acc: 0.986250, Time 00:03:41, lr: 0.010000\n",
      "Epoch 33, train_loss: 0.069411, train_acc: 0.977725, Time 00:03:41, lr: 0.010000\n",
      "Epoch 34, train_loss: 0.032159, train_acc: 0.990450, Time 00:03:41, lr: 0.010000\n",
      "Epoch 35, train_loss: 0.045275, train_acc: 0.985750, Time 00:03:41, lr: 0.010000\n",
      "Epoch 36, train_loss: 0.050171, train_acc: 0.984550, Time 00:03:41, lr: 0.010000\n",
      "Epoch 37, train_loss: 0.040194, train_acc: 0.987500, Time 00:03:41, lr: 0.010000\n",
      "Epoch 38, train_loss: 0.043385, train_acc: 0.986050, Time 00:03:41, lr: 0.010000\n",
      "Epoch 39, train_loss: 0.047288, train_acc: 0.984275, Time 00:03:41, lr: 0.010000\n",
      "Epoch 40, train_loss: 0.038117, train_acc: 0.987875, Time 00:03:41, lr: 0.010000\n",
      "Epoch 41, train_loss: 0.042224, train_acc: 0.986725, Time 00:03:41, lr: 0.010000\n",
      "Epoch 42, train_loss: 0.036804, train_acc: 0.988300, Time 00:03:41, lr: 0.010000\n",
      "Epoch 43, train_loss: 0.037116, train_acc: 0.988500, Time 00:03:41, lr: 0.010000\n",
      "Epoch 44, train_loss: 0.038391, train_acc: 0.987700, Time 00:03:41, lr: 0.010000\n",
      "Epoch 45, train_loss: 0.036507, train_acc: 0.988525, Time 00:03:41, lr: 0.010000\n",
      "Epoch 46, train_loss: 0.031419, train_acc: 0.990175, Time 00:03:41, lr: 0.010000\n",
      "Epoch 47, train_loss: 0.037163, train_acc: 0.987950, Time 00:03:40, lr: 0.010000\n",
      "Epoch 48, train_loss: 0.032104, train_acc: 0.989325, Time 00:03:40, lr: 0.010000\n",
      "Epoch 49, train_loss: 0.026794, train_acc: 0.991700, Time 00:03:40, lr: 0.010000\n",
      "Epoch 50, train_loss: 0.008767, train_acc: 0.997475, Time 00:03:44, lr: 0.001000\n",
      "Epoch 51, train_loss: 0.002291, train_acc: 0.999600, Time 00:03:41, lr: 0.001000\n",
      "Epoch 52, train_loss: 0.001684, train_acc: 0.999750, Time 00:03:41, lr: 0.001000\n",
      "Epoch 53, train_loss: 0.001340, train_acc: 0.999750, Time 00:03:41, lr: 0.001000\n",
      "Epoch 54, train_loss: 0.001155, train_acc: 0.999750, Time 00:03:42, lr: 0.001000\n",
      "Epoch 55, train_loss: 0.001068, train_acc: 0.999750, Time 00:03:40, lr: 0.001000\n",
      "Epoch 56, train_loss: 0.000970, train_acc: 0.999775, Time 00:03:40, lr: 0.001000\n",
      "Epoch 57, train_loss: 0.001028, train_acc: 0.999750, Time 00:03:41, lr: 0.001000\n",
      "Epoch 58, train_loss: 0.000933, train_acc: 0.999725, Time 00:03:40, lr: 0.001000\n",
      "Epoch 59, train_loss: 0.000832, train_acc: 0.999825, Time 00:03:41, lr: 0.001000\n",
      "Epoch 60, train_loss: 0.000753, train_acc: 0.999800, Time 00:03:41, lr: 0.001000\n",
      "Epoch 61, train_loss: 0.000673, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 62, train_loss: 0.000572, train_acc: 0.999825, Time 00:03:41, lr: 0.001000\n",
      "Epoch 63, train_loss: 0.000491, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 64, train_loss: 0.000465, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 65, train_loss: 0.000778, train_acc: 0.999800, Time 00:03:41, lr: 0.001000\n",
      "Epoch 66, train_loss: 0.000506, train_acc: 0.999825, Time 00:03:41, lr: 0.001000\n",
      "Epoch 67, train_loss: 0.000553, train_acc: 0.999800, Time 00:03:41, lr: 0.001000\n",
      "Epoch 68, train_loss: 0.000506, train_acc: 0.999850, Time 00:03:42, lr: 0.001000\n",
      "Epoch 69, train_loss: 0.000417, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 70, train_loss: 0.000779, train_acc: 0.999750, Time 00:03:41, lr: 0.001000\n",
      "Epoch 71, train_loss: 0.000498, train_acc: 0.999825, Time 00:03:41, lr: 0.001000\n",
      "Epoch 72, train_loss: 0.000410, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 73, train_loss: 0.000396, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 74, train_loss: 0.000386, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 75, train_loss: 0.000384, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 76, train_loss: 0.000360, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 77, train_loss: 0.000381, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 78, train_loss: 0.000362, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 79, train_loss: 0.000382, train_acc: 0.999850, Time 00:03:42, lr: 0.001000\n",
      "Epoch 80, train_loss: 0.000612, train_acc: 0.999800, Time 00:03:41, lr: 0.001000\n",
      "Epoch 81, train_loss: 0.000550, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 82, train_loss: 0.000530, train_acc: 0.999800, Time 00:03:42, lr: 0.001000\n",
      "Epoch 83, train_loss: 0.000385, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 84, train_loss: 0.000329, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 85, train_loss: 0.000344, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 86, train_loss: 0.000359, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 87, train_loss: 0.000332, train_acc: 0.999850, Time 00:03:42, lr: 0.001000\n",
      "Epoch 88, train_loss: 0.000347, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 89, train_loss: 0.000343, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 90, train_loss: 0.000598, train_acc: 0.999775, Time 00:03:41, lr: 0.001000\n",
      "Epoch 91, train_loss: 0.000649, train_acc: 0.999800, Time 00:03:42, lr: 0.001000\n",
      "Epoch 92, train_loss: 0.000374, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 93, train_loss: 0.000340, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 94, train_loss: 0.000320, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 95, train_loss: 0.000325, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 96, train_loss: 0.000331, train_acc: 0.999850, Time 00:03:42, lr: 0.001000\n",
      "Epoch 97, train_loss: 0.000319, train_acc: 0.999875, Time 00:03:41, lr: 0.001000\n",
      "Epoch 98, train_loss: 0.000359, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 99, train_loss: 0.000339, train_acc: 0.999850, Time 00:03:41, lr: 0.001000\n",
      "Epoch 100, train_loss: 0.000290, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 101, train_loss: 0.000289, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 102, train_loss: 0.000292, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103, train_loss: 0.000287, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 104, train_loss: 0.000283, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 105, train_loss: 0.000290, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 106, train_loss: 0.000286, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 107, train_loss: 0.000280, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 108, train_loss: 0.000282, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 109, train_loss: 0.000284, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 110, train_loss: 0.000285, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 111, train_loss: 0.000284, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 112, train_loss: 0.000282, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 113, train_loss: 0.000288, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 114, train_loss: 0.000287, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 115, train_loss: 0.000282, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 116, train_loss: 0.000282, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 117, train_loss: 0.000288, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 118, train_loss: 0.000284, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 119, train_loss: 0.000284, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 120, train_loss: 0.000280, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 121, train_loss: 0.000278, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 122, train_loss: 0.000281, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 123, train_loss: 0.000282, train_acc: 0.999850, Time 00:03:42, lr: 0.000100\n",
      "Epoch 124, train_loss: 0.000282, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 125, train_loss: 0.000284, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 126, train_loss: 0.000283, train_acc: 0.999850, Time 00:03:42, lr: 0.000100\n",
      "Epoch 127, train_loss: 0.000279, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 128, train_loss: 0.000284, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 129, train_loss: 0.000278, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 130, train_loss: 0.000280, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 131, train_loss: 0.000285, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 132, train_loss: 0.000277, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 133, train_loss: 0.000286, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 134, train_loss: 0.000279, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 135, train_loss: 0.000274, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 136, train_loss: 0.000281, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 137, train_loss: 0.000280, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 138, train_loss: 0.000283, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 139, train_loss: 0.000279, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 140, train_loss: 0.000278, train_acc: 0.999875, Time 00:03:42, lr: 0.000100\n",
      "Epoch 141, train_loss: 0.000279, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 142, train_loss: 0.000282, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 143, train_loss: 0.000282, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 144, train_loss: 0.000278, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 145, train_loss: 0.000281, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 146, train_loss: 0.000278, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 147, train_loss: 0.000279, train_acc: 0.999875, Time 00:03:41, lr: 0.000100\n",
      "Epoch 148, train_loss: 0.000281, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 149, train_loss: 0.000280, train_acc: 0.999850, Time 00:03:41, lr: 0.000100\n",
      "Epoch 150, train_loss: 0.000273, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 151, train_loss: 0.000276, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 152, train_loss: 0.000289, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 153, train_loss: 0.000267, train_acc: 0.999925, Time 00:03:41, lr: 0.000010\n",
      "Epoch 154, train_loss: 0.000275, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 155, train_loss: 0.000276, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 156, train_loss: 0.000271, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 157, train_loss: 0.000270, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 158, train_loss: 0.000276, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 159, train_loss: 0.000275, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 160, train_loss: 0.000271, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 161, train_loss: 0.000274, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 162, train_loss: 0.000269, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 163, train_loss: 0.000271, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 164, train_loss: 0.000272, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 165, train_loss: 0.000273, train_acc: 0.999850, Time 00:03:40, lr: 0.000010\n",
      "Epoch 166, train_loss: 0.000271, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 167, train_loss: 0.000272, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 168, train_loss: 0.000270, train_acc: 0.999925, Time 00:03:41, lr: 0.000010\n",
      "Epoch 169, train_loss: 0.000276, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 170, train_loss: 0.000272, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 171, train_loss: 0.000270, train_acc: 0.999875, Time 00:03:42, lr: 0.000010\n",
      "Epoch 172, train_loss: 0.000273, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 173, train_loss: 0.000273, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 174, train_loss: 0.000272, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 175, train_loss: 0.000273, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 176, train_loss: 0.000270, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 177, train_loss: 0.000278, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 178, train_loss: 0.000274, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 179, train_loss: 0.000279, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 180, train_loss: 0.000270, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 181, train_loss: 0.000273, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 182, train_loss: 0.000269, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 183, train_loss: 0.000270, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 184, train_loss: 0.000275, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 185, train_loss: 0.000269, train_acc: 0.999900, Time 00:03:40, lr: 0.000010\n",
      "Epoch 186, train_loss: 0.000269, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 187, train_loss: 0.000271, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 188, train_loss: 0.000275, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 189, train_loss: 0.000269, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 190, train_loss: 0.000277, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 191, train_loss: 0.000272, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 192, train_loss: 0.000275, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 193, train_loss: 0.000272, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 194, train_loss: 0.000269, train_acc: 0.999900, Time 00:03:41, lr: 0.000010\n",
      "Epoch 195, train_loss: 0.000275, train_acc: 0.999850, Time 00:03:41, lr: 0.000010\n",
      "Epoch 196, train_loss: 0.000270, train_acc: 0.999875, Time 00:03:42, lr: 0.000010\n",
      "Epoch 197, train_loss: 0.000272, train_acc: 0.999850, Time 00:03:40, lr: 0.000010\n",
      "Epoch 198, train_loss: 0.000271, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 199, train_loss: 0.000268, train_acc: 0.999875, Time 00:03:41, lr: 0.000010\n",
      "Epoch 200, train_loss: 0.000272, train_acc: 0.999850, Time 00:03:40, lr: 0.000001\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAG2CAYAAADMXWbbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fddda0c73c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = get_net(ctx, num_classes=100)\n",
    "net.hybridize()\n",
    "train(train_data, valid_data, net, ctx, num_epoches, \n",
    "      learning_rate, lr_decay, lr_period, \n",
    "      momentum, weight_decay, cost_period, \n",
    "      print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for data, label in test_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    output = net(data)\n",
    "    preds.extend(output.argsort(axis = 1)[:, -5:].astype(int).asnumpy())\n",
    "\n",
    "filenames = []\n",
    "top_5_labels = []\n",
    "for m in range(len(preds)):\n",
    "    str1 = test_ds.items[m][0][30:]\n",
    "    filenames.append(str1)\n",
    "    str2 = \"\"\n",
    "    for index in reversed(preds[m]):\n",
    "        label = train_ds.synsets[index]\n",
    "        str2 += label\n",
    "    top_5_labels.append(str2)\n",
    "\n",
    "df = pd.DataFrame({'filename': filenames, 'label': top_5_labels})\n",
    "df.to_csv('submission.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame??\n",
    "# pd.DataFrame.to_csv??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = []\n",
    "# top_5_labels = []\n",
    "# for m in range(len(preds)):\n",
    "#     str1 = test_ds.items[m][0][30:]\n",
    "#     filenames.append(str1)\n",
    "#     str2 = \"\"\n",
    "#     for index in reversed(preds[m]):\n",
    "#         label = train_ds.synsets[index]\n",
    "#         str2 += label\n",
    "#     top_5_labels.append(str2)\n",
    "\n",
    "# print(filenames[500],top_5_labels[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_ds.items[399])\n",
    "# print(train_ds.items[400][0])\n",
    "# print(train_ds.items[400][0][25:])\n",
    "# print(test_ds.items[0])\n",
    "# print(test_ds.items[0][0])\n",
    "# print(test_ds.items[0][0][30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = []\n",
    "# # a =nd.array([[2, 5, 1, 3, 4, 8, 10]])\n",
    "# a =nd.array([[2, 5, 1, 3, 4, 8, 10], [3, 7, 5, 8, 10, 12, 51]])\n",
    "# print(a)\n",
    "# b = a.argsort(axis = 1)[:, -5:].astype(int).asnumpy()\n",
    "# print(b)\n",
    "# pred.extend(b)\n",
    "# print(len(pred))\n",
    "# print(type(np.array(pred[1])))\n",
    "# print(np.array(pred))\n",
    "# print(output.shape)\n",
    "# print(len(preds[100]))\n",
    "# for i in reversed(preds[1]):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  # label for directory in disk\n",
    "# def label_of_directory(directory):\n",
    "#     \"\"\"\n",
    "#     sorted for label indices\n",
    "#     return a dict for {'classes', 'range(len(classes))'}\n",
    "#     \"\"\"\n",
    "#     classes = []\n",
    "#     for subdir in sorted(os.listdir(directory)):\n",
    "#         if os.path.isdir(os.path.join(directory, subdir)):\n",
    "#             classes.append(subdir)\n",
    "\n",
    "#     num_classes = len(classes)\n",
    "#     class_indices = dict(zip(classes, range(len(classes))))\n",
    "#     return class_indices\n",
    "\n",
    "# # get key from value in dict\n",
    "# def get_key_from_value(dict, index):\n",
    "#     for keys, values in dict.items():\n",
    "#         if values == index:\n",
    "#             return keys\n",
    "\n",
    "# # geneartor list of image list in test\n",
    "# def generator_list_of_imagepath(path):\n",
    "#     image_list = []\n",
    "#     for image in os.listdir(path):\n",
    "#         if not image == '.DS_Store':\n",
    "#             image_list.append(path + image)\n",
    "#     return image_list\n",
    "\n",
    "# # read image and resize to gray\n",
    "# def load_image(image):\n",
    "#     img = Image.open(image)\n",
    "#     img = img.resize((128, 128))\n",
    "#     img = np.array(img)\n",
    "#     img = img / 255\n",
    "#     img = img.reshape((1,) + img.shape + (1,))  # reshape img to size(1, 128, 128, 1)\n",
    "#     return img\n",
    "\n",
    "# # get label of model predict test image top_1 preidct\n",
    "# def get_label_predict_top1(image, model):\n",
    "#     \"\"\"\n",
    "#     image = load_image(image), input image is a ndarray\n",
    "#     retturn best of label\n",
    "#     \"\"\"\n",
    "#     predict_proprely = model.predict(image)\n",
    "#     predict_label = np.argmax(predict_proprely, axis=1)\n",
    "#     return predict_label\n",
    "\n",
    "# # get label of model predict test image top_k predict\n",
    "# def get_label_predict_top_k(image, model, top_k):\n",
    "#     \"\"\"\n",
    "#     image = load_image(image), input image is a ndarray\n",
    "#     return top-5 of label\n",
    "#     \"\"\"\n",
    "#     # array 2 list\n",
    "#     predict_proprely = model.predict(image)\n",
    "#     predict_list = list(predict_proprely[0])\n",
    "#     min_label = min(predict_list)\n",
    "#     label_k = []\n",
    "#     for i in range(top_k):\n",
    "#         label = np.argmax(predict_list)\n",
    "#         predict_list.remove(predict_list[label])\n",
    "#         predict_list.insert(label, min_label)\n",
    "#         label_k.append(label)\n",
    "#     return label_k\n",
    "\n",
    "# # test image predict best label from model\n",
    "# def test_image_predict_top1(model, test_image_path, directory):\n",
    "\n",
    "#     model.load_weights(WEIGHTS_PATH)\n",
    "#     image_list = generator_list_of_imagepath(test_image_path)\n",
    "\n",
    "#     predict_label = []\n",
    "#     class_indecs = label_of_directory(directory)\n",
    "#     for image in image_list:\n",
    "#         img = load_image(image)\n",
    "#         label_index = get_label_predict_top1(img, model)\n",
    "#         label = get_key_from_value(class_indecs, label_index)\n",
    "#         predict_label.append(label)\n",
    "\n",
    "#     return predict_label\n",
    "\n",
    "# # test image predict top-5 label from model\n",
    "# def test_image_predict_top_k(modle, test_image_path, directory, top_k):\n",
    "\n",
    "#     model.load_weights(WEIGHTS_PATH)\n",
    "#     image_list = generator_list_of_imagepath(test_image_path)\n",
    "\n",
    "#     predict_label = []\n",
    "#     class_indecs = label_of_directory(directory)\n",
    "#     for image in image_list:\n",
    "#         img = load_image(image)\n",
    "#         # return a list of label max->min\n",
    "#         label_index = get_label_predict_top_k(img, model, 5)\n",
    "#         label_value_dict = []\n",
    "#         for label in label_index:\n",
    "#             label_value = get_key_from_value(class_indecs, label)\n",
    "#             label_value_dict.append(str(label_value))\n",
    "\n",
    "#         predict_label.append(label_value_dict)\n",
    "\n",
    "#     return predict_label\n",
    "\n",
    "# # translate list to str in label\n",
    "# def tran_list2str(predict_list_label):\n",
    "#     new_label = []\n",
    "#     for row in range(len(predict_list_label)):\n",
    "#         str = \"\"\n",
    "#         for label in predict_list_label[row]:\n",
    "#             str += label\n",
    "#         new_label.append(str)\n",
    "#     return new_label\n",
    "\n",
    "# # save filename , lable as csv\n",
    "# def save_csv(test_image_path, predict_label):\n",
    "#     image_list = generator_list_of_imagepath(test_image_path)\n",
    "#     save_arr = np.empty((10000, 2), dtype=np.str)\n",
    "#     save_arr = pd.DataFrame(save_arr, columns=['filename', 'label'])\n",
    "#     predict_label = tran_list2str(predict_label)\n",
    "#     for i in range(len(image_list)):\n",
    "#         filename = image_list[i].split('/')[-1]\n",
    "#         save_arr.values[i, 0] = filename\n",
    "#         save_arr.values[i, 1] = predict_label[i]\n",
    "#     save_arr.to_csv('submit_test.csv', decimal=',', encoding='utf-8', index=False, index_label=False)\n",
    "# print('submit_test.csv have been write, locate is :', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
